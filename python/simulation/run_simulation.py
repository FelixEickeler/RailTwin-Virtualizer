#  31.01.2023 ----------------------------------------------------------------------------------------------------------------------#  created by: Felix Eickeler#              felix.eickeler@tum.de       # ----------------------------------------------------------------------------------------------------------------------------------import datetimeimport osimport shutilimport subprocessimport sysimport argparseimport uuidfrom operator import itemgetterfrom pathlib import Pathimport randomimport pandasimport numpy as npimport xml.etree.ElementTree as ETimport timepyhelios_folder = Path("~/helios++").expanduser()sys.path.append(pyhelios_folder.__str__())script_folder = Path("/home/phaethon/scripts")clean_up = Truefrom xml.dom import minidomclass SpeedoMeter:    def __init__(self):        self.speeds = [self.new_speed()]        self.speed_size = len(self.speeds)        self.change_speed()        self._idx = 0    def change_speed(self):        self.speed_size = random.randrange(10, 100)        self.speeds = np.linspace(self.speeds[-1], self.new_speed(), self.speed_size)    def new_speed(self):        return (random.random() * 10 + 60) / 3.6    def __iter__(self):        return self    def __next__(self):        self._idx += 1        if self._idx >= len(self.speeds):            self.change_speed()            self._idx = 0        return self.speeds[self._idx]def last(collection):    if hasattr(collection, '__reversed__'):        last = next(reversed(collection))    else:        for last in collection:            pass    return lastdef prelaunch(obj_paths: [str]):    for obj_path in obj_paths:        output_folder = (obj_path.parent / "helios")        output_folder.mkdir(parents=True, exist_ok=True)        scene_name = obj_path.stem.replace("#", "")        # copy_platform.xml        _platform_path = script_folder / "templates/railtwin_platforms.xml"        platform_path = output_folder.parent.parent / _platform_path.name        shutil.copy(_platform_path, platform_path)        del _platform_path        # copy_platform.xml        _scanner_path = script_folder / "templates/railtwin_scanners.xml"        scanner_path = output_folder.parent.parent / _scanner_path.name        shutil.copy(_scanner_path, scanner_path)        del _scanner_path        # alter scene.xml        scene_path = script_folder / "templates/railtwin_scene.xml"        with open(scene_path, "r") as scene_xml:            file = scene_xml.read()        file = file.replace("#scene_name", f"{scene_name}_scene")        file = file.replace("#mtl_src", obj_path.with_suffix(".mtl").__str__())        file = file.replace("#obj_src", obj_path.with_suffix(".obj").__str__())        scene_path = output_folder / "scene.xml"        with open(scene_path, "w") as scene:            scene.write(file)        # create one survey copy it then modify        # check for blender created files:        trajectories = output_folder.parent.glob("*_local.csv")        # print(list(trajectories))        if not trajectories:            trajectories = output_folder.parent.glob("*.csv")            print("Falling back to global")        for trajectory_path in trajectories:            survey_basename = trajectory_path.stem            survey_str = assemble_survey(trajectory_csv=trajectory_path, template_path=script_folder / "templates/railtwin_survey.xml")            survey_output = output_folder / survey_basename            survey_output.mkdir(parents=True, exist_ok=True)            # create platform            for current_platform in ["vmx-rail-left", "vmx-rail-middle", "vmx-rail-right"]:                survey_name = f"{survey_basename}_{current_platform.split('-')[-1]}"                current_survey = survey_str.replace("#survey_name", survey_name)                current_survey = current_survey.replace("#scene_src", f"{scene_path}#{scene_name}_scene")                current_survey = current_survey.replace("#platform_src", f"{platform_path.__str__()}#{current_platform.__str__()}")                current_survey = current_survey.replace("#scanners_src", scanner_path.__str__())                print(survey_output / f"survey_{survey_name}.xml")                with open(survey_output / f"survey_{survey_name}.xml", "w") as f:                    f.write(current_survey)def chunkify(arr, items):    for i in range(0, len(arr), items): yield arr[i:i + items]def assemble_survey(trajectory_csv, template_path):    csv_dtypes = {        "x": float, "y": float, "z": float,        "horizontal_distance": float, "segment_horizontal": int, "horizontal_type": int, "segment_vertical": float,        "segment_type": str    }    waypoints = pandas.read_csv(trajectory_csv, dtype=csv_dtypes)    waypoints.drop_duplicates(subset=["x"], inplace=True, ignore_index=True)    waypoints.sort_values(by=["horizontal_distance"], inplace=True, ignore_index=True)    speedo = SpeedoMeter()    puls_freq = [9e5, 1.5e6, 2.25e6, 3.e6][random.randint(0, 3)] / 3    # puls_freq = 1e5 / 3    scan_freq = [150, 200, 250][random.randint(0, 2)]    xml = ET.parse(template_path)    survey_node = xml.getroot()[0]    profile_id = f"{uuid.uuid4()}"    platformSettings = ET.SubElement(xml.getroot(), "scannerSettings", attrib={        "id": profile_id,        "active": "true",        "pulseFreq_hz": str(int(puls_freq)),        # "scanAngle_deg": "true",        # "headRotateAxis": "y",        "verticalAngleMin_deg": "0.0",        "verticalAngleMax_deg": "360",        "scanFreq_hz": str(int(scan_freq))}                                     )    for row_id, waypoint in waypoints.iterrows():        leg = ET.SubElement(survey_node, "leg")        platformSettings = ET.SubElement(leg, "platformSettings", attrib={            "x": str(waypoint["x"]),            "y": str(waypoint["y"]),            "z": str(waypoint["z"]),            "movePerSec_m": str(int(np.round(next(speedo), 1))),            "smoothTurn": "false"})        scannerSettings = ET.SubElement(leg, "scannerSettings", attrib={            "template": profile_id,            "trajectoryTimeInterval_s": "0.05"        })    return minidom.parseString(ET.tostring(xml.getroot())).toprettyxml(indent="   ")def simulate_surveys(surveys, gps_time_tracker=None, failed_tracker=None, timing_path=None, recursive=True):    if failed_tracker is None:        failed_tracker = {}    if gps_time_tracker is None:        gps_time_tracker = {}    if timing_path is None:        if len(surveys) > 0:            timing_path = surveys[0].parent.parent        else:            return failed_tracker    timings = {}    for survey_path in surveys:        bp = survey_path.stem.replace("survey_", "")        result_folder = survey_path.parent / bp        lck_file_path = survey_path.parent / f"success_{bp}.lck"        if result_folder.exists() and lck_file_path.exists():            print(f"Skipping {bp} !")            continue        timings[survey_path.stem] = {"start": time.time(), "end": time.time() - 1}        if survey_path.parent not in gps_time_tracker:            gps_time_tracker[survey_path.parent] = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())        args = [helios_runtime.__str__(), survey_path.__str__(),                "--output", survey_path.parent.__str__(),                "--seed", "41170534",                "--njobs", "32",                "--gpsStartTime", gps_time_tracker[survey_path.parent]]        try:            subprocess.run(args, check=True, timeout=3600)            failed = False        except KeyboardInterrupt:            print("Manually_skipped")            failed = True        except subprocess.TimeoutExpired:            print("Skipped due to estimated freeze")            failed = time.gmtime(0)        diff_time = timings[survey_path.stem]["end"] - timings[survey_path.stem]["start"]        print(f"The simulation of {bp}, took {diff_time} and {'True' if not failed else 'False'}")        if failed:            try:                failed_tracker[survey_path] += 1            except KeyError:                failed_tracker[survey_path] = 1        else:            open(lck_file_path, "w").close()            timings[survey_path.stem]["end"] = time.time()    with open(timing_path / f"timings.csv", "a+") as t:        stimes = sorted(timings.items(), key=itemgetter(0))        for i, st in enumerate(stimes):            start = st[1]["start"]            end = st[1]["end"]            t.write("{}\t {}\t{}\t{}\t{}\n".format(i, st[0],                                                   datetime.datetime.fromtimestamp(end - start).strftime("%H:%M:%S"),                                                   datetime.datetime.fromtimestamp(start).strftime("%Y-%m-%d %H:%M:%S"),                                                   datetime.datetime.fromtimestamp(end).strftime("%Y-%m-%d %H:%M:%S")                                                   )                    )            # create success lck file    if len(failed_tracker) > 0:        print("Some entities could not be processed. Recursive solution !")        print(failed_tracker)        simulate_surveys(failed_tracker.keys(), gps_time_tracker, failed_tracker)    else:        items = sorted(failed_tracker.items(), key=itemgetter(1))        for k, v in items:            print(f"{k} \t- failcounter  {v}")    return failed_trackerif __name__ == "__main__":    parser = argparse.ArgumentParser(description='Launch from Docker')    parser.add_argument('--input_path', type=Path, help='Path to survey')    parser.add_argument('--task', type=str, help="[pre, run, post]", default="post")    parser.add_argument('--output_path', type=Path, help="[pre, run, post]", default=None)    parser.add_argument('--combine_n', type=int, default=0, help="Recombine how many files in post, this should be typically a multiplication")    parser.add_argument('--final', dest='combine_points', help="If you need a final pointcloud. This might impact drive capacity !",                        action='store_true', default=False)    from_main_script = parser.parse_args()    from_main_script.input_path = from_main_script.input_path.expanduser()    # print(sys.argv)    # print(from_main_script.input_path)    sim_paths = [p for p in from_main_script.input_path.glob("**/*.obj")]    if from_main_script.task == "pre":        prelaunch(sim_paths)        print("DOCKER: Preparation completed...")    elif from_main_script.task == "run":        print("Starting the simulation...")        helios_runtime = Path("~/helios++/_build/helios").expanduser()        gps_time_tracker = {}        surveys = [p for p in from_main_script.input_path.glob("**/*.xml") if p.stem.find("survey_") >= 0]        simulate_surveys(surveys, gps_time_tracker)    elif from_main_script.task == "post":        print("Postprocessing the results")        surveys = [p for p in from_main_script.input_path.glob("**/*.xml") if p.__str__().find("survey_") >= 0]        # merge into chunks (controlled by cobine_n)        all_combine = {}        for survey_path in surveys:            dom = minidom.parse(survey_path.__str__())            survey_name = dom.getElementsByTagName('survey')[0].getAttribute('name')            try:                points_path = last((survey_path.parent / "Survey Playback" / survey_name).iterdir()) / "points"                print(f"Old path structure: {points_path}")            except FileNotFoundError:  # new layout ?                points_path = last((survey_path.parent / f"{survey_name}").iterdir())            xyz = list(points_path.glob("*.xyz"))            scanner_orientation = survey_path.stem.split('_')[-1]            print(f"Post processing the output of {points_path}")            output_path = survey_path.parent            # make sure the number is increasing            xyz.sort()            nr_of_file2combine = len(xyz) if from_main_script.combine_n == 0 else from_main_script.combine_n            chunk_folder = output_path / "chunks"            chunk_folder.mkdir(exist_ok=True)            for chunk_id, to_be_combined in enumerate(chunkify(xyz, nr_of_file2combine)):                cupath = chunk_folder / f"points_{scanner_orientation}_{chunk_id}.xyz"                cupath.parent.mkdir(exist_ok=True)                with open(cupath, 'wb') as outfile:                    for filename in to_be_combined:                        with open(filename, 'rb') as readfile:                            shutil.copyfileobj(readfile, outfile)            trajectory = points_path.glob("*.txt")            helios_trajectory_path = survey_path.parent / "trajectory.txt"            # no leverarm included in helios, maybe later this will be added for now only execute once            # traj_path = output_path / f"trajectory_{scanner_orientation}.txt"            traj_path = output_path / f"trajectory.txt"            if not traj_path.exists():                with open(traj_path, 'wb') as outfile:                    for filename in trajectory:                        with open(filename, 'rb') as readfile:                            shutil.copyfileobj(readfile, outfile)        combine = set()        # determine real surveys (left, middle, right)        for survey_path in surveys:            combine.add(survey_path.parent)        # for each "real" survey        for path in combine:            t1 = time.time()            print(f"Merging to finalize {path.parent.stem}")            if from_main_script.combine_points:                point_chunks = list((path / "chunks").glob("*.xyz"))                point_chunks.sort()                point_chunks.sort(key=lambda x: x.__str__().split("_")[-1])                scanner_positions = []                current_outpath = path / f"combined_{path.stem}.xyz"                alignment_nr = 0                for sim_path in sim_paths:                    if current_outpath.parent.is_relative_to(sim_path.parent):                        try:                            all_combine[sim_path.parent].append(current_outpath)                        except:                            all_combine[sim_path.parent] = [current_outpath]                        alignment_nr = len(all_combine[sim_path.parent]) - 1                        break                with open(current_outpath, 'w') as outfile:                    for filename in point_chunks:                        scanner_pos = filename.stem.split("_")[-2]                        try:                            sid = scanner_positions.index(scanner_pos)                        except ValueError:                            sid = len(scanner_positions)                            scanner_positions.append(scanner_pos)                        with open(filename, 'r') as readfile:                            while lines := readfile.readlines(1000000):                                out_lines = []                                for line in lines:                                    X, Y, Z, intensity, echoWidth, returnNumber, numberOfReturns, fullwaveIndex, hitObjectId, _class, gpsTime = line.split()                                    out_lines.append(" ".join([X, Y, Z, intensity, _class, gpsTime, str(sid), str(alignment_nr)]) + "\n")                                outfile.writelines(out_lines)                    with open(path / "classifications.log", "w") as csf:                        csf.write("scanner_id scanner_pos\n")                        for sid, name in enumerate(scanner_positions):                            csf.write(f"{sid} {name}\n")            t2 = time.time()            print(f"... took {t2 - t1}")            # shutil.copyfileobj(readfile, outfile)        for model_folder, alignment_folders in all_combine.items():            alignment_numbering = []            ll = model_folder / f'{model_folder.stem}.xyz'            print(f"Generating: {ll}")            with open(model_folder / f"{model_folder.stem}.xyz", 'wb') as outfile:                for af in alignment_folders:                    print(f"Adding Content of {af}")                    with open(af, 'rb') as readfile:                        shutil.copyfileobj(readfile, outfile)                    if clean_up:                        os.remove(af)            with open(model_folder / "alignment.log", "w") as csf:                csf.write("Alignment ID Track\n")                for sid, name in enumerate(alignment_folders):                    csf.write(f"{sid} {name.stem}\n")